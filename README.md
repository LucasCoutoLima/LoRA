# LoRA - Fine Tuning

Esse projeto foi feito para a disciplina de mestrado da Unicamp IA024 - Redes Neurais Profundas para Processamento de Linguagem Natural.

Neste projeto, foram adicionadas as matrizes A e B (LoRA) em todas as camadas lineares e embeddings de um modelo GPT-2 para Fine Tuning eficiente para a tarefa de predição de próxima palavra.

A base de dados utilizada foram textos retirados de livros do Machado de Assis (https://github.com/ethelbeluzzi/projetomachado)

Acurácia, Loss e Perplexidade foram avaliadas.
